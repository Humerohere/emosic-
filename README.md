Emotion-Music-Recommendation
Recommending music based on your facial expressions using the FER 2013 dataset and VGG19 model.

Demo
(Forgive the image quality and my expressions ðŸ˜†)

Project Description
The emotion recognition model uses the VGG19 model trained on the FER 2013 dataset. It can detect 7 emotions. The project works by getting a live video feed from the webcam, passing it through the model to get a prediction of emotion. According to the predicted emotion, the app will fetch playlists of songs from music files, recommend the songs by displaying them on the screen, and play the music as well.

Features
Real-time expression detection and song recommendations.
Playlists fetched from files and music can be played using the play-music-api.
Skeumorphism UI for the website.
Running the App
Flask
Run pip install -r requirements.txt to install all dependencies.
In Spotipy.py, enter your credentials generated by your Spotify Developer account in 'auth_manager'. Note: This is only required if you want to update recommendation playlists. Also, uncomment the import statement in 'camera.py'.
Run python app.py and give camera permission if asked.
Tech Stack
TensorFlow
Keras
Spotipy
Tkinter (For testing)
Flask
scikit-learn
Pygame
pandas
numpy
Jupyter Notebook
Dataset
The dataset used for this project is the FER2013 dataset. Models trained on this dataset can classify 7 emotions. The dataset can be found here.

Note that the dataset is highly imbalanced, with the happy class having the maximum representation. This might result in okayish accuracy after training.

Model Architecture
The model uses VGG19, a pre-trained Convolutional Neural Network:

VGG19 Backbone: The VGG19 model, pre-trained on ImageNet, is used without the top fully connected layers.
Global Average Pooling: Applied after the convolutional base.
Dense Layers: Additional Dense layers for classification:
Dense layer with 4096 units and ReLU activation with L2 regularization.
Output Dense layer with 7 units and softmax activation for classifying 7 emotions.
Training Setup:
Loss function: Categorical Crossentropy.
Optimizer: Adam with a learning rate of 0.0001.
Metrics: Accuracy.
Training
The images were normalized, resized to (48, 48), and converted to grayscale in batches of 64 using ImageDataGenerator in Keras. The training took around 20 minutes and was trained on Kaggle for 6 epochs, achieving an accuracy of approximately 80%.

Confusion Matrix and Classification Report
To evaluate the model performance, confusion matrices and classification reports were generated using scikit-learn.

Current Condition
The entire project works perfectly fine. Live detection provides good frame rates due to multithreading.

